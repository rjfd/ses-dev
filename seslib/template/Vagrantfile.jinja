# -*- mode: ruby -*-
# vi: set ft=ruby :


def common_provision(node, hostname, num_nodes)
  (1..num_nodes).each do |n|
    ipsuf = 200 + n
    node.vm.provision "shell", inline: <<-SHELL
    echo "192.168.100.#{ipsuf} node#{n} node#{n}.oa.local" >> /etc/hosts
    SHELL
  end

  node.vm.provision "shell", inline: <<-SHELL
    zypper ar https://download.opensuse.org/distribution/leap/15.1/repo/oss/ leap15.1
    zypper ar https://download.opensuse.org/repositories/filesystems:/ceph:/nautilus/openSUSE_Leap_15.1/ nautilus-repo
    # zypper ar http://download.suse.de/ibs/SUSE:/SLE-15-SP1:/Update:/Products:/SES6/standard/ nautilus-repo
    # zypper ar https://download.opensuse.org/repositories/home:/rjdias:/branches:/filesystems:/ceph:/nautilus/openSUSE_Leap_15.1/ nautilus-repo-home
    # zypper ar https://download.opensuse.org/repositories/home:/dmdiss:/tcmu-runner-1.3/openSUSE_Leap_15.0/home:dmdiss:tcmu-runner-1.3.repo
    zypper mr -p 70 nautilus-repo
    # zypper mr -p 60 nautilus-repo-home
    zypper --gpg-auto-import-keys ref

    # Install ceph
    zypper -n install ceph-base ceph-mon vim git iputils hostname jq

  SHELL

  node.vm.provision "shell", inline: <<-SHELL
    echo "192.168.100.200 salt salt.oa.local" >> /etc/hosts
    sleep 10
    cat /home/vagrant/.ssh/id_rsa.pub >> /home/vagrant/.ssh/authorized_keys
    [ ! -e "/root/.ssh" ] && mkdir /root/.ssh
    chmod 600 /home/vagrant/.ssh/id_rsa
    cp /home/vagrant/.ssh/id_rsa* /root/.ssh/
    cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
    hostnamectl set-hostname #{hostname}

  SHELL
end


Vagrant.configure("2") do |config|
  config.ssh.insert_key = false
  config.vm.box = "opensuse-leap-15.1"

  config.vm.provider "libvirt" do |lv|
    if settings.has_key?('libvirt_host') then
      lv.host = settings['libvirt_host']
    end
    if settings.has_key?('libvirt_user') then
      lv.username = settings['libvirt_user']
    end
    if settings.has_key?('libvirt_use_ssl') then
      lv.connect_via_ssh = true
    end

    lv.memory = settings.has_key?('vm_memory') ? settings['vm_memory'] : 4096
    lv.cpus = settings.has_key?('vm_cpus') ? settings['vm_cpus'] : 2
    if settings.has_key?('vm_storage_pool') then
      lv.storage_pool_name = settings['vm_storage_pool']
    end
    lv.nic_model_type = "e1000"

    lv.cpu_mode = 'host-passthrough'
  end
  config.vm.provider :virtualbox do |vb|
    vb.memory = settings.has_key?('vm_memory') ? settings['vm_memory'] : 4096
    vb.cpus = settings.has_key?('vm_cpus') ? settings['vm_cpus'] : 2
  end

  (1..num_nodes).each do |n|
    config.vm.define :"node#{n}" do |node|
      ipsuf = 200 + n
      node.vm.network :private_network, ip: "192.168.100.#{ipsuf}"
      node.vm.network :private_network, ip: "192.168.170.#{ipsuf}"

      node.vm.provision "file", source: "keys/id_rsa",
                                destination:".ssh/id_rsa"
      node.vm.provision "file", source: "keys/id_rsa.pub",
                                destination:".ssh/id_rsa.pub"

      node.vm.synced_folder ".", "/vagrant", disabled: true

      node.vm.provider "libvirt" do |lv|
        (1..num_volumes).each do |d|
          lv.storage :file, size: volume_size, type: 'qcow2'
        end
      end
      node.vm.provider :virtualbox do |vb|
        for i in 1..num_volumes do
          file_to_disk = "./disks/#{node.vm.hostname}-disk#{i}.vmdk"
          unless File.exist?(file_to_disk)
            vb.customize ['createmedium', 'disk', '--filename', file_to_disk,
              '--size', volume_size]
            vb.customize ['storageattach', :id,
              '--storagectl', 'SATA Controller',
              '--port', i, '--device', 0,
              '--type', 'hdd', '--medium', file_to_disk]
          end
        end
      end

      hostname = "node#{n}"
      common_provision(node, hostname, num_nodes)

      node.vm.provision "shell", inline: <<-SHELL
        zypper -n install salt-minion
        systemctl enable salt-minion
        systemctl start salt-minion

        touch /tmp/ready
      SHELL
    end
  end

  config.vm.define :salt do |salt|
    # salt.vm.hostname = "salt.oa.local"
    salt.vm.network :private_network, ip: "192.168.100.200"

    salt.vm.provision "file", source: "keys/id_rsa",
                              destination:".ssh/id_rsa"
    salt.vm.provision "file", source: "keys/id_rsa.pub",
                              destination:".ssh/id_rsa.pub"

    salt.vm.provision "file", source: "keys/iscsi-gateway.crt",
                              destination:"server.crt"
    salt.vm.provision "file", source: "keys/iscsi-gateway.key",
                              destination:"server.key"

    salt.vm.provision "file", source: "bin/", destination:"/home/vagrant/"

    salt.vm.synced_folder deepsea_repo, '/home/vagrant/DeepSea', type: 'nfs',
                            :nfs_export => nfs_auto_export,
                            :mount_options => ['nolock,vers=3,udp,noatime,actimeo=1,port=4049'],
                            :linux__nfs_options => ['rw','no_subtree_check','all_squash','insecure']

    salt.vm.synced_folder ".", "/vagrant", disabled: true

    roles = {
        "mon" => "node*",
        "igw" => ("node[12]*" if num_nodes > 1) || "node*",
        "rgw" => ("node[13]*" if num_nodes > 2) || "node*",
        "mds" => ("node[23]*" if num_nodes > 2) || "node*",
        "ganesha" => ("node[23]*" if num_nodes > 2) || "node*",
        "mgr" => ("node[12]*" if num_nodes > 1) || "node*"
    }

    common_provision(salt, "salt", num_nodes)

    salt.vm.provision "shell", inline: <<-SHELL
#      zypper ar https://download.opensuse.org/repositories/filesystems:/ceph:/nautilus/openSUSE_Leap_15.1/ nautilus-repo
#      # zypper ar https://download.opensuse.org/repositories/home:/rjdias:/branches:/filesystems:/ceph:/nautilus/openSUSE_Leap_15.1/ nautilus-repo-home
#      # zypper ar http://download.suse.de/ibs/SUSE:/SLE-15-SP1:/Update:/Products:/SES6/standard/ nautilus-repo
#      zypper mr -p 70 nautilus-repo
#      # zypper mr -p 60 nautilus-repo-home
#      zypper --gpg-auto-import-keys ref
#
#      zypper -n install salt-minion salt-master make
#      systemctl enable salt-master
#      systemctl start salt-master
#      sleep 5
#      systemctl enable salt-minion
#      systemctl start salt-minion
#
#      while : ; do
#        PROVISIONED_NODES=`ls -l /tmp/ready-* 2>/dev/null | wc -l`
#        echo "waiting for node1, node2 and node3 (${PROVISIONED_NODES}/3)";
#        [[ "${PROVISIONED_NODES}" != "3" ]] || break
#        sleep 2;
#        scp -o StrictHostKeyChecking=no node2:/tmp/ready /tmp/ready-node1;
#        scp -o StrictHostKeyChecking=no node2:/tmp/ready /tmp/ready-node2;
#        scp -o StrictHostKeyChecking=no node3:/tmp/ready /tmp/ready-node3;
#      done
#
#       sleep 10
#       salt-key -Ay
#
#       sleep 5
#       cd /home/vagrant/DeepSea
#       make install
#
##        deepsea config /Updates/auto_reboot disable
##        deepsea config /Updates/kernel disable
##        deepsea config /Updates/package disable
##        deepsea config /Global_Options/DEV_ENV enable
##        deepsea config /Ceph_Dashboard/password set admin
##        deepsea config /Ceph_Dashboard/ssl enable
##        #deepsea config /Ceph_Dashboard/ssl/cert_file set /home/vagrant/server.crt
##        #deepsea config /Ceph_Dashboard/ssl/cert_key_file set /home/vagrant/server.key
##        deepsea config ls
#
#       cat > /srv/salt/ceph/updates/default_my.sls <<EOF
#dummy command:
#  test.nop
#EOF
#
#       cp /srv/salt/ceph/updates/default_my.sls /srv/salt/ceph/updates/restart
#       sed -i 's/default/default_my/g' /srv/salt/ceph/updates/init.sls
#       sed -i 's/default/default_my/g' /srv/salt/ceph/updates/restart/init.sls
#       cp /srv/salt/ceph/updates/default_my.sls /srv/salt/ceph/time
#       sed -i 's/default/default_my/g' /srv/salt/ceph/time/init.sls
#
#       echo "***** RUNNING stage.0 *******"
#       # salt-run state.orch ceph.stage.0
#       deepsea stage run --simple-output ceph.stage.0
#       sleep 5
#
#        cat > /tmp/policy.cfg <<EOF
## Cluster assignment
#cluster-ceph/cluster/*.sls
## Hardware Profile
#profile-default/cluster/*.sls
#profile-default/stack/default/ceph/minions/*yml
#role-storage/cluster/node*.sls
## Common configuration
#config/stack/default/global.yml
#config/stack/default/ceph/cluster.yml
## Role assignment
#role-master/cluster/salt*.sls
#role-admin/cluster/salt*.sls
#role-mon/cluster/#{roles['mon']}.sls
#role-igw/cluster/#{roles['igw']}.sls
#role-rgw/cluster/#{roles['rgw']}.sls
#role-mds/cluster/#{roles['mds']}.sls
#role-ganesha/cluster/#{roles['ganesha']}.sls
#role-mgr/cluster/#{roles['mgr']}.sls
#role-prometheus/cluster/salt*.sls
#role-grafana/cluster/salt*.sls
#EOF
#
#       echo "***** RUNNING stage.1 *******"
#       # salt-run state.orch ceph.stage.1
#       deepsea stage run --simple-output ceph.stage.1
#       sleep 5
#
#       cp /tmp/policy.cfg /srv/pillar/ceph/proposals/
#
#       echo "***** RUNNING stage.2 *******"
#       # salt-run state.orch ceph.stage.2
#       deepsea stage run --simple-output ceph.stage.2
#       sleep 5
#
#       echo "***** RUNNING stage.3 *******"
#       # DEV_ENV=true salt-run state.orch ceph.stage.3
#       DEV_ENV=true deepsea stage run --simple-output ceph.stage.3
#       sleep 5
#
#       echo "***** RUNNING stage.4 *******"
#       #salt-run state.orch ceph.stage.4
#       deepsea stage run --simple-output ceph.stage.4
#
      SHELL
  end
end
